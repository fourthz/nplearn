<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Fisher-Irwin Test • nplearn</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="The Fisher-Irwin Test">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">nplearn</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased version">0.0.0.9017</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Bernoulli-Events.html">Bernoulli Events</a>
    </li>
    <li>
      <a href="../articles/binomdist.html">The Binomial Distribution</a>
    </li>
    <li>
      <a href="../articles/FisherIrwin.html">The Fisher-Irwin Test</a>
    </li>
    <li>
      <a href="../articles/gof.html">Goodness of Fit</a>
    </li>
    <li>
      <a href="../articles/lsprop.html">Large-Sample Proportion Inference</a>
    </li>
    <li>
      <a href="../articles/OnePropExact.html">Exact Inference for a Single Proportion</a>
    </li>
    <li>
      <a href="../articles/ProdBinom.html">The Product Binomial</a>
    </li>
    <li>
      <a href="../articles/randomization.html">Randomization Tests</a>
    </li>
    <li>
      <a href="../articles/releffic.html">Relative Efficiency</a>
    </li>
    <li>
      <a href="../articles/ReplaceScores.html">Replacement Scores</a>
    </li>
    <li>
      <a href="../articles/signtest.html">The Sign Test</a>
    </li>
    <li>
      <a href="../articles/Workflow-Sample.html">Workflow Sample</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>The Fisher-Irwin Test</h1>
            
      
      
      <div class="hidden name"><code>FisherIrwin.Rmd</code></div>

    </div>

    
    
<p>When we have two independent samples and a dichotomous response variable, our interest is in the probability of a joint event in which there are some number of successes in one sample and some number of successes in the other sample. Other than the sample size, there is no restriction on how many successes can occur in each sample. In some situations, however, the total number of successes is known in advance. In that case, the number of successes in our two conditions are dependent on one another. Knowing the number of successes in one condition will tell us the number in the other condition. In this vignette, we turn our attention to analyzing data when the number of successes is known prior to the start of the study.</p>
<div id="required-packages" class="section level3">
<h3 class="hasAnchor">
<a href="#required-packages" class="anchor"></a>Required packages</h3>
<p>The packages required for this vignette are nplearn and MASS. Make certain that you have installed these packages before attempting to load the libraries.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(nplearn)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(MASS)</span></code></pre></div>
</div>
<div id="the-hypergeometric-distribution" class="section level3">
<h3 class="hasAnchor">
<a href="#the-hypergeometric-distribution" class="anchor"></a>The hypergeometric distribution</h3>
<p>In the <em>product binomial</em> vignette, we analyzed data obtained from two samples created by randomizing 20 band students to morning and afternoon tryouts for the all-state band. Let’s again consider this scenario, but this time suppose that these students all come from one school. The all-state band committee has indicated how many students can be accepted from each school, based on school size. For this school, only five slots can be filled by band members from the school.</p>
<p>Here are the (hypothetical) data.</p>
<div class="figure">
<img src="FisherIrwin%20Band%20Results.jpg" alt=""><p class="caption"><em>Figure 1</em></p>
</div>
<p>As in the previous example, 20 students were randomly assigned to morning and afternoon tryout conditions. The difference this time is that we know before the tryouts even begin that there will be 5 successes. Notice that with this knowledge we can fill in every margin in our table before we ever collect the data. This is clearly not a product binomial problem because the number of possible successes do not vary from 0 to 10 for each condition. The highest number of successes that can occur in a condition is 5. Further, the number of successes that can occur in the other condition is directly related to how many there are in the first condition so that the numbers of successes must sum to 5.</p>
<p>We can conceive of the possibilities by focusing on just one cell in this four cell crossbreak table. Let’s focus on the upper left cell (morning successes). This can range from 0 to 5 successes. Notice that whatever value we set this cell to be in that range, the entire table can be completed. We already know that we cannot calculate the probability of this table with the product binomial formula, so what do we do?</p>
<p>The answer is that under the null hypothesis of equal proportions of successes in both morning and afternoon conditions, the probability of a particular value of <span class="math inline">\(\hat{\delta}\)</span> is given by the hypergeometric formula. We can use a function in R to make this calculation. For example, for our oberved data in Figure 1, here is the probability calculation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Hypergeometric">dhyper</a></span>(<span class="dv">4</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co">#&gt; [1] 0.1354489</span></span></code></pre></div>
<p>The entry of values is for obtaining the probability of randomly selecting 4 morning students out of 10 morning students when we also have 10 afternoon students and there will be 5 slots to fill.</p>
<p>Realizing that number of morning successes can only range from 0 to 5, and also knowing that the remainder of the success will be in the afternoon, we can create the entire probability distribution.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>morning.success &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">5</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>afternoon.success &lt;-<span class="st"> </span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span>morning.success</span>
<span id="cb3-3"><a href="#cb3-3"></a>delta.hat &lt;-<span class="st"> </span>morning.success<span class="op">/</span><span class="dv">10</span> <span class="op">-</span><span class="st"> </span>afternoon.success<span class="op">/</span><span class="dv">10</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>prob &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Hypergeometric">dhyper</a></span>(morning.success, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/cbind">cbind</a></span>(morning.success, afternoon.success, delta.hat, prob)</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">#&gt;      morning.success afternoon.success delta.hat       prob</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">#&gt; [1,]               0                 5      -0.5 0.01625387</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">#&gt; [2,]               1                 4      -0.3 0.13544892</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">#&gt; [3,]               2                 3      -0.1 0.34829721</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">#&gt; [4,]               3                 2       0.1 0.34829721</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">#&gt; [5,]               4                 1       0.3 0.13544892</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">#&gt; [6,]               5                 0       0.5 0.01625387</span></span></code></pre></div>
<p>Here’s a check that these probabilities sum to 1.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(prob)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>Suppose we are looking for judging bias in either direction (i.e. favoring either the morning or afternoon). Then we need to obtain our <em>p</em> value we need to add up all the probabilities for absolute values of <span class="math inline">\(\hat{\delta}\)</span> that are at least as great as our observed value of 0.3.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(prob[<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/MathFun">abs</a></span>(delta.hat) <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.3</span>])</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co">#&gt; [1] 0.3034056</span></span></code></pre></div>
<p>Clearly this is insufficient evidence to make claims of judging bias. If we had set out to study whether there is bias in favor of morning sessions, we would have just looked at one side of the distribution.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(prob[delta.hat <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.3</span>])</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co">#&gt; [1] 0.1517028</span></span></code></pre></div>
<p>This still is probably not enough evidence for us to make bias accusations, even though we did find that 40% of the morning students and only 10% of the afternoon students were selected. The sample size is simply not large enough for us to claim bias, even with this estimate of time-of-day effect.</p>
</div>
<div id="a-test-for-median-equality" class="section level3">
<h3 class="hasAnchor">
<a href="#a-test-for-median-equality" class="anchor"></a>A test for median equality</h3>
<p>When the response variable for a single sample yields quantitative data, we can use the sign test as a test of a hypothesis about the median. Similarly, the Fisher-Irwin test can be used as a test of a hypothesis about the equality of two medians.</p>
<p>To exemplify this method, consider an example in which an independent evaluation firm interviews employees at 12 companies and then, based on the results of the interview, gives each company an “employee satisfaction score.” Some of these companies have put an employee feedback system (EFS) in place as a means for employees to communicate concerns to management. The primary question is whether companies with such systems have higher satisfaction scores than those without such systems. We can view these companies as representing a larger population of companies, some of which have EFS and some which do not. Our interest is in whether the median satisfaction score is greater for companies with feedback systems than for companies that have not implemented such a system. Let’s input the scores for this hypothetical example.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>efs &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a>no.efs &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span></code></pre></div>
<p>Here are the null and alternative hypotheses.</p>
<p><span class="math inline">\(H_0: \theta_{efs} = \theta_{no \: efs}\)</span></p>
<p><span class="math inline">\(H_a: \theta_{efs} &gt; \theta_{no \: efs}\)</span></p>
<p>If the null hypothesis is true, the two types of companies share the same median, so to estimate this common median we can combine all scores.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>all.companies &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(efs, no.efs)</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/median">median</a></span>(all.companies)</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co">#&gt; [1] 7.5</span></span></code></pre></div>
<p>Let us now look at how many companies are above the median in each of the two conditions.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(efs <span class="op">&gt;</span><span class="st"> </span><span class="fl">7.5</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="co">#&gt; [1] 6</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sum">sum</a></span>(no.efs <span class="op">&gt;</span><span class="st"> </span><span class="fl">7.5</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<p>With this information in hand, we can construct the following table.</p>
<div class="figure">
<img src="efs%20example.jpg" alt=""><p class="caption"><em>Figure 2</em></p>
</div>
<p>As with the previous Fisher-Irwin example, all margin totals are known before the interviews are conducted and the companies are scored. We know which of the the companies have EFS in place, so the two sample sizes are known. The fact that we have hypothesized a common median tells us that half of the companies will be above the median and half will be below the median, so this margin is known as well. We can think of being above the median as a “success” and being below as a “failure,” so this is the same type of situation as we had in the Fisher-Irwin example, so we can use the hypergeometric distribution.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>efs.above &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">6</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>no.efs.above &lt;-<span class="st"> </span><span class="dv">6</span> <span class="op">-</span><span class="st"> </span>efs.above</span>
<span id="cb10-3"><a href="#cb10-3"></a>delta.hat &lt;-<span class="st"> </span>efs.above<span class="op">/</span><span class="dv">7</span> <span class="op">-</span><span class="st"> </span>no.efs.above<span class="op">/</span><span class="dv">5</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>prob &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/Hypergeometric">dhyper</a></span>(efs.above, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">6</span>)</span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/cbind">cbind</a></span>(efs.above, no.efs.above, delta.hat, prob)</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">#&gt;      efs.above no.efs.above  delta.hat        prob</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">#&gt; [1,]         1            5 -0.8571429 0.007575758</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">#&gt; [2,]         2            4 -0.5142857 0.113636364</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="co">#&gt; [3,]         3            3 -0.1714286 0.378787879</span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co">#&gt; [4,]         4            2  0.1714286 0.378787879</span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co">#&gt; [5,]         5            1  0.5142857 0.113636364</span></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co">#&gt; [6,]         6            0  0.8571429 0.007575758</span></span></code></pre></div>
<p>This is a one-sided test, so we can see that for the observed difference in proportions of 0.86, the <em>p</em> value is 0.0076. We can declare that EFS makes a difference in employee satisfaction, though there is a caveat: the data are from an observational study, rather than from an experiment. Extraneous variables may be responsible for the satisfaction differences. For example, suppose that the same companies that care enough about employee opinion to solicit feedback are also the companies that care enough to implement other employee-focused initiatives, such as break facility improvements or bonus incentives. These may be responsible for the difference, rather than the EFS.</p>
<p>Instead of looking at the full distribution, we can use a function that is available in R.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>efs.table &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/matrix">matrix</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>),</span>
<span id="cb11-2"><a href="#cb11-2"></a>                    <span class="dt">nrow =</span> <span class="dv">2</span>,</span>
<span id="cb11-3"><a href="#cb11-3"></a>                    <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/fisher.test">fisher.test</a></span>(efs.table, <span class="dt">alternative =</span> <span class="st">"greater"</span>)</span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="co">#&gt; </span></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="co">#&gt;  Fisher's Exact Test for Count Data</span></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="co">#&gt; </span></span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="co">#&gt; data:  efs.table</span></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co">#&gt; p-value = 0.007576</span></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="co">#&gt; alternative hypothesis: true odds ratio is greater than 1</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span id="cb11-13"><a href="#cb11-13"></a><span class="co">#&gt;  2.4008    Inf</span></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="co">#&gt; sample estimates:</span></span>
<span id="cb11-15"><a href="#cb11-15"></a><span class="co">#&gt; odds ratio </span></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="co">#&gt;        Inf</span></span></code></pre></div>
<p>We observe that the <em>p</em> value matches what we calculated earlier. We also notice a confidence interval for the “odds ratio,” so now would be a good time to learn about this statistic.</p>
</div>
<div id="the-odds-ratio" class="section level3">
<h3 class="hasAnchor">
<a href="#the-odds-ratio" class="anchor"></a>The odds ratio</h3>
<p>In the previous example, the estimated proportion of EFS companies that are above the median company satisfaction score is 6/7, or 0.86. For the non-EFS companies it is 0/5, or 0. The odds of a specified outcome is the ratio of the expected frequency of that outcome to that of another outcome. In the current example, this would be the expected frequency of a company being above the median to that of being below the median. Obviously we don’t know this expected frequency. It is a population value. What we do know, however are the observed frequencies, so we can use these to estimate the population values.</p>
<p>The odds of an EFS company being above the median are estimated to be 6 to 1, because 6 companies are above the median and 1 is below. If we consider this as a ratio, this is 6/1, or 6. This means that there is a six times greater chance for an EFS company being above the median than below the median.</p>
<p>Estimating the odds for a non-EFS company gives us a peculiar result: It is 0 to 5, or 0/5, or 0. Stating that there is a “0 times greater chance” of being above the median than below the median is likely due to the small sample size. We would assume that with a larger number of companies, we would find some of them above the median satisfaction score, even if they had not implemented a feedback system.</p>
<p>The ratio of the odds, better known as the <em>odds ratio</em> is quite simply a ratio of these two odds. In the current example, we have (6/1)/(0/5). Again, we get a peculiar result due to the small sample size: 6/0. Dividing a number by 0 gives us an infinite result, which is why the odds ratio reported by the function above is listed as “inf”. Even with this peculiar result, the confidence interval for the odds ratio was calculated as 2.4 to infinity. This means we are 95% confident that for the population of companies, implementation of an EFS makes it between 2.4 times to “infinite times” more likely to result in a satisfaction score that is above the median satisfaction score of all companies.</p>
<p>Let’s do this again with the band data so that we can see an example without the peculiarity of having “infinite” in our results. Remember that this was a two-sided test because we were looking for judging bias in either direction.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>morning.odds &lt;-<span class="st"> </span><span class="dv">4</span><span class="op">/</span><span class="dv">6</span></span>
<span id="cb12-2"><a href="#cb12-2"></a>afternoon.odds &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">9</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>odds.ratio &lt;-<span class="st"> </span>morning.odds<span class="op">/</span>afternoon.odds</span>
<span id="cb12-4"><a href="#cb12-4"></a>odds.ratio</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co">#&gt; [1] 6</span></span></code></pre></div>
<p>This tells us that based on our observed data, we estimate that the chance of being selected for the all-state band is 6 times greater if you are judged in the morning than if you are judged in the afternoon. This is substantial, but remember that with the small sample size, this was not enough evidence to conclude that this is judging bias. Let’s look at it with the R function.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>band.table &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/matrix">matrix</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">9</span>),</span>
<span id="cb13-2"><a href="#cb13-2"></a>                     <span class="dt">nrow =</span> <span class="dv">2</span>,</span>
<span id="cb13-3"><a href="#cb13-3"></a>                     <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/fisher.test">fisher.test</a></span>(band.table)</span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="co">#&gt; </span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co">#&gt;  Fisher's Exact Test for Count Data</span></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="co">#&gt; </span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co">#&gt; data:  band.table</span></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co">#&gt; p-value = 0.3034</span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="co">#&gt; alternative hypothesis: true odds ratio is not equal to 1</span></span>
<span id="cb13-12"><a href="#cb13-12"></a><span class="co">#&gt; 95 percent confidence interval:</span></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="co">#&gt;    0.4075889 326.8267297</span></span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="co">#&gt; sample estimates:</span></span>
<span id="cb13-15"><a href="#cb13-15"></a><span class="co">#&gt; odds ratio </span></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="co">#&gt;   5.485861</span></span></code></pre></div>
<p>You may notice that the estimate of the odds ratio, about 5.5, is different than the odds ratio that we obtained of 6. Why? The sample odds ratio (our simple calculation above) can be calculated regardless of the number of marginal values that are fixed. That is, it does not “know” that we specified that there are only 5 slots available for the all-state band. If the judges changed it to be 6 slots, we would still get the same estimate (6) of the odds ratio. Yet how can this be? If there are fewer slots, the odds should change! The way to deal with this is to calculate an odds ratio that is conditioned on our fixed margins. Showing how to do this “by hand” is beyond what were we are going to go, but that’s OK because the R function is doing exactly that for us. The estimate it reports is actually a better estimate than the simple one we calculated above, so we just need to focus on interpretation. The chance of success in a morning slot is about 5.5 times the chance of success in an afternoon slot.</p>
<p>Look at the confidence interval. It ranges from saying that the chance of success in the morning is less than half the chance of success in the afternoon all the way to the chance in the morning being 327 times the chance of success in the afternoon! This huge interval is consistent with our large <em>p</em> value. Clearly we could benefit from a larger sample.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Michael Seaman.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
